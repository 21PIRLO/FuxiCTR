{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune a model via grid search\n",
    "\n",
    "FuxiCTR version: v1.0\n",
    "\n",
    "This tutorial shows how to tune model hyper-parameters via grid search over the specified tuning space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "We provide a useful tool script `run_param_tuner.py` to tune FuxiCTR models based on YAML config files.\n",
    "\n",
    "+ --config: The config file that defines the tuning space\n",
    "+ --gpu: The available gpus for parameters tuning and multiple gpus can be used (e.g., using --gpu 0 1 for two gpus)\n",
    "+ --tag: (optional) Specify the tag to determine which expid to run (e.g. 001 for the first expid). This is useful to rerun one specific experiment_id that contains the tag.\n",
    "\n",
    "In the following example, we use the hyper-parameters of `FM_test` in [./config](https://github.com/xue-pai/FuxiCTR/tree/main/config) as the base setting, and create a tuner config file `FM_tuner_config.yaml` in [benchmarks/tuner_config](https://github.com/xue-pai/FuxiCTR/tree/main/benchmarks/tuner_config), which defines the tuning space for parameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FM_tuner_config.yaml\n",
    "base_config: ../config/ # the location of base config\n",
    "base_expid: FM_test # the expid of default hyper-parameters\n",
    "dataset_id: taobao_tiny_data # the dataset_id used\n",
    "\n",
    "tuner_space:\n",
    "    model_root: './tuner_config/' # the value will override the default value in FM_test\n",
    "    embedding_dim: [16, 32] # the values in the list will be grid-searched\n",
    "    regularizer: [0, 1.e-6, 1.e-5] # the values in the list will be grid-searched\n",
    "    learning_rate: 1.e-3 # it is equivalent to [1.e-3]\n",
    "    batch_size: 128 # the value will override the default value in FM_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, if a key in `tuner_space` has values stored in a list, those values will be grid-searched. Otherwise, the default value in `FM_test` will be applied.\n",
    "\n",
    "Run the following command to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd benchmarks\n",
    "!python run_param_tuner.py --config ./tuner_config/FM_tuner_config.yaml --gpu 0 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finished, all the searched results can be accessed from `FM_tuner_config.csv` in the `./benchmarks` folder.\n",
    "\n",
    "Note that if you want to run only one group of hyper-parameters in the search space, you can use `--tag` to specify which one to run. In the following example, 001 means the expid (i.e., FM_test_001_7f7f3b34) corresponding to the first group of hyper-parameters. It is useful when one needs to rerun an expid for reproduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd benchmarks\n",
    "!python run_param_tuner.py --config ./tuner_config/FM_tuner_config.yaml --tag 001 --gpu 0 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above example config file shows how to import base_expid and dataset_id from the base_config folder, it is also flexible to directly expand the base setting in the tunner config file. Both configurations are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example load base_expid and dataset_id from the same file\n",
    "base_expid: FM_test # the expid of default hyper-parameters\n",
    "dataset_id: taobao_tiny_data # the dataset_id used\n",
    "\n",
    "model_config:\n",
    "    FM_test:\n",
    "        model_root: '../checkpoints/'\n",
    "        workers: 3\n",
    "        verbose: 1\n",
    "        patience: 2\n",
    "        pickle_feature_encoder: True\n",
    "        use_hdf5: True\n",
    "        save_best_only: True\n",
    "        every_x_epochs: 1\n",
    "        debug: False\n",
    "        model: FM\n",
    "        dataset_id: taobao_tiny_data\n",
    "        loss: binary_crossentropy\n",
    "        metrics: ['logloss', 'AUC']\n",
    "        task: binary_classification\n",
    "        optimizer: adam\n",
    "        learning_rate: 1.0e-3\n",
    "        regularizer: 1.e-8\n",
    "        batch_size: 128\n",
    "        embedding_dim: 4\n",
    "        epochs: 1\n",
    "        shuffle: True\n",
    "        seed: 2019\n",
    "        monitor: 'AUC'\n",
    "        monitor_mode: 'max'\n",
    "    \n",
    "dataset_config:\n",
    "    taobao_tiny_data:\n",
    "        data_root: ../data/\n",
    "        data_format: csv\n",
    "        train_data: ../data/tiny_data/train_sample.csv\n",
    "        valid_data: ../data/tiny_data/valid_sample.csv\n",
    "        test_data: ../data/tiny_data/test_sample.csv\n",
    "        min_categr_count: 1\n",
    "        feature_cols:\n",
    "            - {name: [\"userid\",\"adgroup_id\",\"pid\",\"cate_id\",\"campaign_id\",\"customer\",\"brand\",\"cms_segid\",\n",
    "                      \"cms_group_id\",\"final_gender_code\",\"age_level\",\"pvalue_level\",\"shopping_level\",\"occupation\"], \n",
    "                      active: True, dtype: str, type: categorical}\n",
    "        label_col: {name: clk, dtype: float}\n",
    "\n",
    "tuner_space:\n",
    "    model_root: './tuner_config/' # the value will override the default value in FM_test\n",
    "    embedding_dim: [16, 32] # the values in the list will be grid-searched\n",
    "    regularizer: [0, 1.e-6, 1.e-5] # the values in the list will be grid-searched\n",
    "    learning_rate: 1.e-3 # it is equivalent to [1.e-3]\n",
    "    batch_size: 128 # the value will override the default value in FM_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "If you want to find more running examples, please refer to the benchmarking results in the [BARS-CTR-Prediction](https://openbenchmark.github.io/ctr-prediction) benchmark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
